{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df20706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from encoder import label_encoder\n",
    "import pickle\n",
    "import json\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb41ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76294fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect(text):\n",
    "    spell = SpellChecker()\n",
    "    \n",
    "    low = wordpunct_tokenize(text)\n",
    "    corrected = \"\"\n",
    "    punctuations = \"\"\"!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "    \n",
    "    for word in low:\n",
    "        if word not in punctuations:\n",
    "            if (corrected != \"\") and (corrected[-1] != \"'\"):\n",
    "                corrected += \" \" + spell.correction(word)\n",
    "            else:\n",
    "                corrected += spell.correction(word)\n",
    "        else:\n",
    "            corrected += word\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf74a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasSymptoms(text):\n",
    "    low = text.replace(',',' ').split()\n",
    "    df = pd.read_csv(\"Database/Symptom-severity.csv\")\n",
    "    df['Symptom'] = df['Symptom'].str.replace(' ','')\n",
    "    df['Symptom'] = df['Symptom'].str.replace('_',' ')\n",
    "    possibility = 0\n",
    "    for word in low:\n",
    "        similarities = {}\n",
    "        for symptom in df['Symptom'].values:\n",
    "            similarities[symptom] = nlp(word).similarity(nlp(symptom))\n",
    "        possibility = max(max(similarities.values()),possibility)\n",
    "    return possibility > 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d44db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(text):\n",
    "\n",
    "    stop_words = ['a',\n",
    "                     'about',\n",
    "                     'above',\n",
    "                     'after',\n",
    "                     'again',\n",
    "                     'against',\n",
    "                     'ain',\n",
    "                     'all',\n",
    "                     'am',\n",
    "                     'an',\n",
    "                     'any',\n",
    "                     'are',\n",
    "                     'aren',\n",
    "                     \"aren't\",\n",
    "                     'as',\n",
    "                     'be',\n",
    "                     'because',\n",
    "                     'been',\n",
    "                     'before',\n",
    "                     'being',\n",
    "                     'below',\n",
    "                     'between',\n",
    "                     'both',\n",
    "                     'but',\n",
    "                     'by',\n",
    "                     'can',\n",
    "                     'couldn',\n",
    "                     \"couldn't\",\n",
    "                     'd',\n",
    "                     'did',\n",
    "                     'didn',\n",
    "                     \"didn't\",\n",
    "                     'do',\n",
    "                     'does',\n",
    "                     'doesn',\n",
    "                     \"doesn't\",\n",
    "                     'doing',\n",
    "                     'don',\n",
    "                     \"don't\",\n",
    "                     'down',\n",
    "                     'during',\n",
    "                     'each',\n",
    "                     'few',\n",
    "                     'for',\n",
    "                     'from',\n",
    "                     'further',\n",
    "                     'had',\n",
    "                     'hadn',\n",
    "                     \"hadn't\",\n",
    "                     'has',\n",
    "                     'hasn',\n",
    "                     \"hasn't\",\n",
    "                     'have',\n",
    "                     'haven',\n",
    "                     \"haven't\",\n",
    "                     'having',\n",
    "                     'he',\n",
    "                     'her',\n",
    "                     'here',\n",
    "                     'hers',\n",
    "                     'herself',\n",
    "                     'him',\n",
    "                     'himself',\n",
    "                     'his',\n",
    "                     'how',\n",
    "                     'i',\n",
    "                     'if',\n",
    "                     'into',\n",
    "                     'is',\n",
    "                     'isn',\n",
    "                     \"isn't\",\n",
    "                     'it',\n",
    "                     \"it's\",\n",
    "                     'its',\n",
    "                     'itself',\n",
    "                     'just',\n",
    "                     'll',\n",
    "                     'm',\n",
    "                     'ma',\n",
    "                     'me',\n",
    "                     'mightn',\n",
    "                     \"mightn't\",\n",
    "                     'more',\n",
    "                     'most',\n",
    "                     'mustn',\n",
    "                     \"mustn't\",\n",
    "                     'my',\n",
    "                     'myself',\n",
    "                     'needn',\n",
    "                     \"needn't\",\n",
    "                     'no',\n",
    "                     'nor',\n",
    "                     'not',\n",
    "                     'now',\n",
    "                     'o',\n",
    "                     'of',\n",
    "                     'off',\n",
    "                     'once',\n",
    "                     'only',\n",
    "                     'or',\n",
    "                     'other',\n",
    "                     'our',\n",
    "                     'ours',\n",
    "                     'ourselves',\n",
    "                     'out',\n",
    "                     'over',\n",
    "                     'own',\n",
    "                     're',\n",
    "                     's',\n",
    "                     'same',\n",
    "                     'shan',\n",
    "                     \"shan't\",\n",
    "                     'she',\n",
    "                     \"she's\",\n",
    "                     'should',\n",
    "                     \"should've\",\n",
    "                     'shouldn',\n",
    "                     \"shouldn't\",\n",
    "                     'so',\n",
    "                     'some',\n",
    "                     'such',\n",
    "                     't',\n",
    "                     'than',\n",
    "                     'that',\n",
    "                     \"that'll\",\n",
    "                     'the',\n",
    "                     'their',\n",
    "                     'theirs',\n",
    "                     'them',\n",
    "                     'themselves',\n",
    "                     'then',\n",
    "                     'there',\n",
    "                     'these',\n",
    "                     'they',\n",
    "                     'this',\n",
    "                     'those',\n",
    "                     'through',\n",
    "                     'to',\n",
    "                     'too',\n",
    "                     'under',\n",
    "                     'until',\n",
    "                     'up',\n",
    "                     've',\n",
    "                     'very',\n",
    "                     'was',\n",
    "                     'wasn',\n",
    "                     \"wasn't\",\n",
    "                     'we',\n",
    "                     'were',\n",
    "                     'weren',\n",
    "                     \"weren't\",\n",
    "                     'what',\n",
    "                     'when',\n",
    "                     'where',\n",
    "                     'which',\n",
    "                     'while',\n",
    "                     'who',\n",
    "                     'whom',\n",
    "                     'why',\n",
    "                     'will',\n",
    "                     'with',\n",
    "                     'won',\n",
    "                     \"won't\",\n",
    "                     'wouldn',\n",
    "                     \"wouldn't\",\n",
    "                     'y',\n",
    "                     'you',\n",
    "                     \"you'd\",\n",
    "                     \"you'll\",\n",
    "                     \"you're\",\n",
    "                     \"you've\",\n",
    "                     'your',\n",
    "                     'yours',\n",
    "                     'yourself',\n",
    "                     'yourselves']\n",
    "    \n",
    "    punctuations = \"\"\"!\"#$%&'()*+ -./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "    \n",
    "\n",
    "    word_tokens = wordpunct_tokenize(text)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if (not w.lower() in stop_words) and (w not in punctuations)]\n",
    "    \n",
    "    return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532c6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symptomize(text):\n",
    "    los = text.replace(' and ',' , ').split(',')\n",
    "    df = pd.read_csv(\"Database/Symptom-severity.csv\")\n",
    "    df['Symptom'] = df['Symptom'].str.replace(' ','')\n",
    "    df['Symptom'] = df['Symptom'].str.replace('_',' ')\n",
    "    \n",
    "    formatted = []\n",
    "    \n",
    "    for each in los:\n",
    "        if each[0] == \" \":\n",
    "            each = each[1:]\n",
    "        if each[-1] == \" \":\n",
    "            each = each[:-1]\n",
    "        similarities = {}\n",
    "        for symptom in df['Symptom'].values:\n",
    "            similarities[symptom] = nlp(each).similarity(nlp(symptom))\n",
    "        keymax = \"Null\" if max(similarities.values()) == 0 else max(similarities, key= lambda x: similarities[x])\n",
    "        if keymax != \"Null\":\n",
    "            formatted.append(keymax.replace(' ','_'))\n",
    "    \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7621713",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"training_data\",\"rb\"))\n",
    "\n",
    "words = data[\"words\"]\n",
    "classes = data[\"classes\"]\n",
    "train_x = data[\"train_x\"]\n",
    "train_y = data[\"train_y\"]\n",
    "\n",
    "\n",
    "with open(\"intents.json\") as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1803c7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tflearn\\initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Restoring parameters from D:\\My Documents\\COLLEGE\\Major Project\\Code\\ML Model\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# Loading TFLearn Model\n",
    "\n",
    "\n",
    "net = tflearn.input_data(shape = [None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,len(train_y[0]),activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net,tensorboard_dir='tflearn_logs')\n",
    "model.load('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82052f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1252221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    \n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence,words,show_details=False):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    \n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"found in bag : %s\" % w)\n",
    "                    \n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eacacf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow(\"Can you help me?\",words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028505d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_THRESHOLD = 0.80\n",
    "def classify(sentence):\n",
    "    results = model.predict([bow(sentence,words)])[0]\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1],reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]],r[1]))\n",
    "        \n",
    "    return return_list\n",
    "\n",
    "def response(sentence,show_details=False):\n",
    "    results = classify(sentence)\n",
    "    \n",
    "    if results:\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    return random.choice(i['responses'])\n",
    "                \n",
    "            results.pop(0)\n",
    "            \n",
    "    else:\n",
    "        for i in intents['intents']:\n",
    "            if i['tag'] == \"error\":\n",
    "                return i['responses'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56fd5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(text):\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Correct spelling errors\n",
    "    text = autocorrect(text)\n",
    "    \n",
    "    output = response(text)\n",
    "    \n",
    "    if output.split()[0] == 'Sorry,':\n",
    "    \n",
    "        if hasSymptoms(text):\n",
    "\n",
    "            # Preprocessing text\n",
    "            text = strip(text)\n",
    "\n",
    "            # Symptom mapper\n",
    "            ls = symptomize(text)\n",
    "\n",
    "            # Encoder\n",
    "            array_of_symptoms = label_encoder(ls)\n",
    "\n",
    "            return array_of_symptoms\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aef1ce63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(\"esdjsk\") == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead5b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [1,2,3]\n",
    "g = str(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d5a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33dd7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174b7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = g.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50652393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', ' 2', ' 3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd432fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "l = [int(x) for x in l]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2fcee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey this is just a random line to check mandate of continuity\n",
      "vaishvik\n",
      "vaishvik\n"
     ]
    }
   ],
   "source": [
    "x = \"global\"\n",
    "\n",
    "def display():\n",
    "    global x\n",
    "    x = \"mondial\"\n",
    "    if True:\n",
    "        print(\"Hey this is just a random line to check mandate of continuity\")\n",
    "        \n",
    "    x = \"vaishvik\"\n",
    "    print(x)\n",
    "\n",
    "display()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2583be8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "deja = list()\n",
    "print(deja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a2d59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "og = [5,6,7,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "989815df",
   "metadata": {},
   "outputs": [],
   "source": [
    "invader = [8,9,10,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4105be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "og.extend(invader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9676252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 0, 0, 0, 0, 8, 9, 10, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort(og)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc24c96",
   "metadata": {},
   "source": [
    "## test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a87396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't understand. Please re-enter your query\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1b6438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreyan Das\\AppData\\Local\\Temp\\ipykernel_8544\\1342047661.py:10: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarities[symptom] = nlp(word).similarity(nlp(symptom))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.543751591610493"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasSymptoms(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2db3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease=\"Fungal infection\"\n",
    "descr = pd.read_csv(\"Database/symptom_Description.csv\")\n",
    "prec = pd.read_csv(\"Database/symptom_precaution.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27069400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          stop irritation\n",
       "1                 Consult nearest hospital\n",
       "2                           apply calamine\n",
       "3                            reduce stress\n",
       "4         wash hands with warm soapy water\n",
       "5                   avoid fatty spicy food\n",
       "6                               cold baths\n",
       "7                 Consult nearest hospital\n",
       "8                            acetaminophen\n",
       "9                                 lie down\n",
       "10                        lie down on side\n",
       "11                              bath twice\n",
       "12                      have balanced diet\n",
       "13        soak affected area in warm water\n",
       "14                              meditation\n",
       "15                  avoid fatty spicy food\n",
       "16                  avoid fatty spicy food\n",
       "17             drink vitamin c rich drinks\n",
       "18                    use neem in bathing \n",
       "19            use heating pad or cold pack\n",
       "20                             eat healthy\n",
       "21                   drink plenty of water\n",
       "22    lie down flat and raise the leg high\n",
       "23                         avoid open cuts\n",
       "24                                 massage\n",
       "25             eat high calorie vegitables\n",
       "26                consult nearest hospital\n",
       "27                              bath twice\n",
       "28                Consult nearest hospital\n",
       "29                              meditation\n",
       "30               switch to loose cloothing\n",
       "31                stop alcohol consumption\n",
       "32                   drink plenty of water\n",
       "33                stop alcohol consumption\n",
       "34                 drink papaya leaf juice\n",
       "35                          consult doctor\n",
       "36                          call ambulance\n",
       "37                          consult doctor\n",
       "38                                exercise\n",
       "39        stop eating solid food for while\n",
       "40                             cover mouth\n",
       "Name: Precaution_1, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec['Precaution_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10087906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
