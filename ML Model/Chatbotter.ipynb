{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e011a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc0b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contextual chat-bot intents\n",
    "import json\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e320d145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi there', 'Is anyone there?', 'Hello', 'Hi'], 'responses': ['Hello there!', 'Hi, how can I help you?', 'Hi!', 'Hello!']}, {'tag': 'options', 'patterns': ['How can you help me?', 'What you can do?', 'I need help', 'What to enter?', 'I want your help', 'I am not feeling well'], 'responses': ['I can help you get an initial diagnosis of for health issues based on your symptoms. Please enter your symptoms to continue']}, {'tag': 'greetings_bye', 'patterns': ['Thanks for your help', 'Bye', 'Goodbye', 'Thanks', 'Thank you', 'okay', 'ok'], 'responses': ['No problem! It was a pleasure helping you', 'Bye-bye! Hope I able to help you :)', 'Goodbye! Hope you to see you again!', 'No problem :) I hope I was able to solve your problem!']}, {'tag': 'information', 'patterns': ['Who are you', 'Are you a doctor', 'Do you have a degree', 'Can you tell me about you', 'How do you know'], 'responses': ['I am Aimee! Your personal healthcare companion!', \"I am not a doctor but an artificial intelligent assistant who can streamline your healthcare journey. I did not take the Hippocratic Oath, but it's my duty :D \", 'While I do not have a degree, I have been trained using a machine learning model using a database that contains information about many diseases. ', 'Hi, I am Aimee! Your personal healthcare companion! Please input your symptoms to know you potential ailments.', \"I have trained for hours, learning about many diseases and their symptoms. Since my prediction might not always be perfect, I can help you to find a real doctor who can examine you. Remember, it's always a good idea to take a second opinion :)\"]}, {'tag': 'general', 'patterns': ['I am ill', ' I am in pain', \"I don't feel good\", 'I am sick', 'I feel low ', 'I want to get better', 'I feel sick, please help'], 'responses': ['Sorry to hear that :( Please enter your symptoms so that I can help you further.']}, {'tag': 'error', 'patterns': [], 'responses': [\"Sorry, I don't understand. Please re-enter your query\"]}]}\n"
     ]
    }
   ],
   "source": [
    "print(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467a07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'list'>\n",
      "{'tag': 'greeting', 'patterns': ['Hi there', 'Is anyone there?', 'Hello', 'Hi'], 'responses': ['Hello there!', 'Hi, how can I help you?', 'Hi!', 'Hello!']}\n",
      "<class 'dict'>\n",
      "greeting\n"
     ]
    }
   ],
   "source": [
    "# Verification that everything has been imported correctly\n",
    "print(type(intents))\n",
    "print(type(intents['intents']))\n",
    "print(intents['intents'][0])\n",
    "print(type(intents['intents'][0]))\n",
    "print(intents['intents'][0]['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cb12f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29  documents\n",
      "5  classes ['general', 'greeting', 'greetings_bye', 'information', 'options']\n",
      "48  unique stemmed words [',', 'a', 'about', 'am', 'anyon', 'ar', 'bet', 'bye', 'can', 'degr', 'do', 'doct', 'ent', 'feel', 'for', 'get', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'how', 'i', 'il', 'in', 'is', 'know', 'low', 'me', \"n't\", 'nee', 'not', 'ok', 'okay', 'pain', 'pleas', 'sick', 'tel', 'thank', 'ther', 'to', 'want', 'wel', 'what', 'who', 'yo', 'you']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        documents.append((w,intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "            \n",
    "# stem words and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "#remove duplicate classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents),\" documents\")\n",
    "print(len(classes), \" classes\",classes)\n",
    "print(len(words),\" unique stemmed words\",words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c96c84f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreyan Das\\AppData\\Local\\Temp\\ipykernel_10436\\743259283.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "output_empty = [0]*len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag,output_row])\n",
    "    \n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f471f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.00882\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1000 | loss: 0.00882 - acc: 0.9999 -- iter: 24/29\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.00895\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1000 | loss: 0.00895 - acc: 0.9999 -- iter: 29/29\n",
      "--\n",
      "INFO:tensorflow:D:\\My Documents\\COLLEGE\\Major Project\\Code\\ML Model\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "net = tflearn.input_data(shape = [None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,len(train_y[0]),activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net,tensorboard_dir='tflearn_logs')\n",
    "model.fit(train_x, train_y,n_epoch=1000,batch_size=8,show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a1da7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({'words':words,'classes':classes,'train_x':train_x,'train_y':train_y},open(\"training_data\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e5d63",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e4846bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06759b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tflearn\\initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Restoring parameters from D:\\My Documents\\COLLEGE\\Major Project\\Code\\ML Model\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(\"training_data\",\"rb\"))\n",
    "\n",
    "words = data[\"words\"]\n",
    "classes = data[\"classes\"]\n",
    "train_x = data[\"train_x\"]\n",
    "train_y = data[\"train_y\"]\n",
    "\n",
    "\n",
    "with open(\"intents.json\") as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "net = tflearn.input_data(shape = [None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,len(train_y[0]),activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net,tensorboard_dir='tflearn_logs')\n",
    "model.load('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75817c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
